\section{Introduction}
\label{sec:introduction}

Nonlinear Dynamical Systems have wide applications in biology, physics, computer science and engineering especially in that of \emph{combinatronics}, which is the area of mathematical studies that focuses on counting to obtain results and specific properties of finite structures.
In simpler terms, finding the best combination of \emph{things}, which will be a term used loosely to describe processes from a birds-eye view, can lead to optimized solutions for problems which simply cannot be solved for in any other way.
 
A clear cut example of such optimization could be seen through the \emph{Knapsack problem}, which given a set of items, their corresponding weights, and a value, the goal is to determine the number of items to include in a collection so that the total weight remains less than or equal to the given limit while still trying to take a value as large as possible. 
The issues that arises within the \emph{Knapsack Problem}, lies when there is some large \emph{n} integer of items to be chosen from, in which trying to compute all the combinations by hand loses efficiency. 
This whole idea of efficiency leads to one of the largest unsolved theoretical hypotheses in computer science called the \emph{P equals NP problem}, where it asks whether every problem whose solution can be quickly checked for correctness can also be computed quickly.
An answer to the \emph{P versus NP} question would open up whether problems that can be verified in polynomial time (\emph{denoted as P}) can also be solved in polynomial time. If it turns out that P $\neq$ NP, which is the more popular belief, it would mean that there are problems in non-deterministic polynomial time (\emph{denoted as NP}) that are harder to solve than to verify: they could not be solved in polynomial time, but the answer could be verified in polynomial time. 

Therefore, a solution to the \emph{Knapsack problem} and other time-consuming problems, is the \emph{genetic algorithm}, which is widely used in stochastic optimization because it is great at taking large, potentially massive search spaces and navigating through them, finding the solutions, to problems you might not find otherwise in a lifetime. It takes the possibility of approaching these \emph{NP} problems, and finding very accurate approximations for them.

\section{The Genetic Algorithm}
\label{sec: the genetic algorithm}

The Genetic Algorithm (GA), is a metaheuristic algorithm inspired by \emph{Charles Darwin's} theory of \emph{Natural Selection}, used for solving Optimization and Search problems in machine learning. This algorithm's strict purpose is to solve for problems that would take a long time to solve, often producing one of the best approximated solutions. The ability to begin solving \emph{NP} problems is just scratching the surface and with this algorithm there is an expectation for more complex systems and hypotheses to be approached in this manner.

\subsection{How it Works}
\label{sec: how it works} 

\textbf{I.} The way the genetic algorithm works is through bits in binary showing membership to a set, where 0 is denoted as not being a member of a particular set, and 1 being a member of a particular set. The use of binary is in efforts to reach a goal to emulate a genetic representation of a solutions domain. 
Before populating a solutions domain, three factors must be determined: how long each bit sequence is, to represent the \emph{n}  amount of \emph{things} that are to be selected from, how many possible solutions or \emph{phenotypes} we want to produced, and a limit that if succeeded will disqualify it as a valid phenotype. For example, the singular binary sequence \emph{1001010111} which signifies a singular possible phenotype, shows a bit length of 10 resembling 10 total \emph{things} within a set, while we see that 6 \emph{things} are members of this phenotype simply by counting the number of 1's that are present in the sequence. It is important to note that the bit sequence length never increases or decreases because there are a fixed amount of items present, and that any number of bits can be flipped to 1 as long as it remains less than or equal to the limit set. Once a limit and integer \emph{n} are chosen, \emph{n} amount of unique phenotypes will be produced of some selected arbritary length \emph{l}. This population of random possible solutions are what we refer to as a \emph{generation} and this first generation is called generation 0 (generations can be tracked by \emph{i-1 generations}, where \emph{i} marks the number of iterations). Furthermore, because all the possible solutions were randomly generated, this evolutionary process begins in complete chaos.

\textbf{II.} Next comes an iterative procedure that will continue to reproduce as long as there are more than one non-zero solutions from the pervious generation. A solution is changed to zero if and only if the total sum of the collection of items succeeds the limit set in part \textbf{I}. Before beginning to reproduce even more different phenotypes, the \emph{Natural Selection} process is introduced, where \emph{fitness, single-point crossovers, and mutations}, all concepts from \emph{Darwin's} Theory of Evolution are assessed. First we proceed to the fitness function, where its purpose is to            
